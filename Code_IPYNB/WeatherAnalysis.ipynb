{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CitiBike Data Analysis. Logistic regression on count of bike trips per day vs. weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.mllib.stat import Statistics as stat\n",
    "from pyspark.sql.functions import desc, asc\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.functions import round, col\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Necessary for distance calculations\n",
    "from math import sin, cos, sqrt, atan2, radians    \n",
    "from pyspark.sql.functions import col, radians, asin, sin, sqrt, cos\n",
    "\n",
    "# set configurations\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# load modules\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.mllib.stat import Statistics as stat\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, when, isnan, count, udf, struct, date_format, coalesce, lit, round\n",
    "from math import sin, cos, sqrt, atan2, radians    \n",
    "from pyspark.sql.functions import col, radians, asin, sin, sqrt, cos\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"weatherwork\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"comm\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"bike\") \\\n",
    "        .getOrCreate()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/project/ds5559/Summer2021_TeamBike/master_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the broadcast variables from csv\n",
    "# duration by station and month\n",
    "durationByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationDuration.csv\"\n",
    "df_durationByStationMonth = spark.read.csv(durationByStationMonth_filepath).rdd.collectAsMap()\n",
    "#for x in list(df_durationByStationMonth)[0:3]:\n",
    "#    print (\"key {}, value {} \".format(x,  df_durationByStationMonth[x]))\n",
    "bc_durationByStationMonth = sc.broadcast(df_durationByStationMonth)\n",
    "# duration by station and month and good/bad weather\n",
    "goodBadDurationByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationWeatherGoodBadDuration.csv\"\n",
    "df_goodBadDurationByStationMonth = spark.read.csv(goodBadDurationByStationMonth_filepath).rdd.collectAsMap()\n",
    "#for x in list(df_goodBadDurationByStationMonth)[0:3]:\n",
    "#    print (\"key {}, value {} \".format(x,  df_goodBadDurationByStationMonth[x]))\n",
    "bc_goodBadDurationByStationMonth = sc.broadcast(df_goodBadDurationByStationMonth)\n",
    "# to add columns with the GOOD and bad weather duration indexes\n",
    "from pyspark.sql.functions import col\n",
    "@udf(returnType=DoubleType())\n",
    "def setweatherGoodDurationIndex(month : str, startStationName : str):\n",
    "    averageDuration = bc_durationByStationMonth.value.get(month + \"-\" + startStationName)\n",
    "    goodDuration= bc_goodBadDurationByStationMonth.value.get(month + \"-\" + startStationName + \"GOOD\")\n",
    "    if goodDuration is None:\n",
    "        # if the dictionary doesn't have a good duration value, use the average trips\n",
    "        goodDuration = averageDuration\n",
    "    index_value = float(goodDuration) / float(averageDuration)\n",
    "    return index_value\n",
    "@udf(returnType=DoubleType())\n",
    "def setweatherBadDurationIndex(month : str, startStationName : str):\n",
    "    averageDuration = bc_durationByStationMonth.value.get(month + \"-\" + startStationName)\n",
    "    badDuration= bc_goodBadDurationByStationMonth.value.get(month + \"-\" + startStationName + \"BAD\")\n",
    "    if badDuration is None:\n",
    "        # if the dictionary doesn't have a bad duration value, use the average trips\n",
    "        badDuration = averageDuration\n",
    "    index_value = float(badDuration) / float(averageDuration)\n",
    "    return index_value\n",
    "# load the standard data file with only the columns I am interested in using:\n",
    "# THE IS THE FINAL DATA FILE\n",
    "file = \"/project/ds5559/Summer2021_TeamBike/master_dataset.parquet\" \n",
    "df = spark.read.parquet(file)\n",
    "df = df.withColumn(\"weatherGoodDurationIndex\", setweatherGoodDurationIndex(col(\"month\"), col(\"startStationName\")))\n",
    "df = df.withColumn(\"weatherBadDurationIndex\", setweatherBadDurationIndex(col(\"month\"), col(\"startStationName\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|usertype|\n",
      "+--------+\n",
      "|   daily|\n",
      "|  annual|\n",
      "|   daily|\n",
      "|   daily|\n",
      "|  annual|\n",
      "|  annual|\n",
      "|  annual|\n",
      "|   daily|\n",
      "|   daily|\n",
      "|  annual|\n",
      "|  annual|\n",
      "|  annual|\n",
      "|   daily|\n",
      "|  annual|\n",
      "|  annual|\n",
      "|  annual|\n",
      "|   daily|\n",
      "|  annual|\n",
      "|   daily|\n",
      "|  annual|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(F.col(\"gender\"))\n",
    "userDict = {'member':'annual','Subscriber':'annual','Customer':'daily'}\n",
    "df = df.replace(userDict,subset=['usertype'])\n",
    "df.select(['usertype']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|usertype|   count|\n",
      "+--------+--------+\n",
      "|   daily| 7124164|\n",
      "|  annual|42270056|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('usertype').count().sort(\"count\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|tripduration_minute|\n",
      "+-------+-------------------+\n",
      "|  count|           49394220|\n",
      "|   mean| 18.305129223284123|\n",
      "| stddev| 204.05231864630437|\n",
      "|    min|           -10008.9|\n",
      "|    max|          165074.55|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"tripduration_minute\", (df.tripduration / 60))\n",
    "\n",
    "df.select('tripduration_minute').describe().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(F.col(\"tripduration\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8861"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(F.col(\"tripduration_minute\") <= 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44387"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col(\"tripduration_minute\") > 480).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"tripduration_minute\", F.when(F.col(\"tripduration_minute\") > 0, F.col(\"tripduration_minute\")).otherwise(\"0\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"tripduration_minute\", F.when(F.col(\"tripduration_minute\") < 480, F.col(\"tripduration_minute\")).otherwise(\"0.0\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-------------------+-------------------+\n",
      "|startRadLat       |endRadLat         |startRadLong       |endRadLong         |\n",
      "+------------------+------------------+-------------------+-------------------+\n",
      "|0.7099873523967277|0.7099873523967277|-1.2912214593940174|-1.2912214593940174|\n",
      "|0.7102313878234002|0.7108809377608699|-1.2910007792652636|-1.2915272517254752|\n",
      "|0.7105313146268409|0.7099391987626652|-1.29093715555631  |-1.2908386765749968|\n",
      "|0.7113661947943947|0.7109956472570291|-1.291032165695735 |-1.291370736008677 |\n",
      "|0.7107304261512315|0.7109660489163764|-1.2916783473222815|-1.2916279617611712|\n",
      "+------------------+------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# radius of earth in miles\n",
    "R = 3963.0\n",
    "\n",
    "#Convert start/end latitude and longitude from degrees to Radians\n",
    "df = df.withColumn(\"startRadLat\", radians(df.startStationLatitude)) \\\n",
    "       .withColumn(\"endRadLat\", radians(df.endStationLatitude)) \\\n",
    "       .withColumn(\"startRadLong\", radians(df.startStationLongitude)) \\\n",
    "       .withColumn(\"endRadLong\", radians(df.endStationLongitude))\n",
    "\n",
    "df.select(df.startRadLat,df.endRadLat,df.startRadLong,df.endRadLong).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"diffRadLat\", (df.startRadLat-df.endRadLat)) \\\n",
    "       .withColumn(\"diffRadLong\", (df.startRadLong-df.endRadLong)) \\\n",
    "       .withColumn(\"diffLat\", (df.startStationLatitude-df.endStationLatitude)) \\\n",
    "       .withColumn(\"diffLong\", (df.startStationLongitude-df.endStationLongitude))\n",
    "\n",
    "df = df.withColumn(\"crowDist\", asin(sqrt((sin(df.diffRadLat/2))**2 + (cos(df.startRadLat) * cos(df.endRadLat)) * (sin(df.diffRadLong/2))**2)) * 2 * R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"verticalDist\", asin(sqrt((sin(df.diffRadLat/2))**2)) * 2 * R)\n",
    "df = df.withColumn(\"horizontalDist\", asin(sqrt((sin(df.diffRadLong/2))**2)) * 2 * R)\n",
    "df = df.withColumn(\"manhattanDist\", (df.horizontalDist + df.verticalDist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(F.col(\"birthyear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.endStationName.isNotNull())\n",
    "df = df.filter(df.endStationLatitude.isNotNull())\n",
    "df = df.filter(df.endStationLongitude.isNotNull())\n",
    "df = df.filter(df.endRadLat.isNotNull())\n",
    "df = df.filter(df.diffRadLat.isNotNull())\n",
    "df = df.filter(df.diffRadLong.isNotNull())\n",
    "df = df.filter(df.diffLat.isNotNull())\n",
    "df = df.filter(df.diffLong.isNotNull())\n",
    "df = df.filter(df.crowDist.isNotNull())\n",
    "df = df.filter(df.verticalDist.isNotNull())\n",
    "df = df.filter(df.horizontalDist.isNotNull())\n",
    "df = df.filter(df.manhattanDist.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_keep = ['date',\n",
    "                'temp',\n",
    "                'feels_like',\n",
    "                'pressure',\n",
    "                'humidity',\n",
    "                'wind_speed',\n",
    "                'rain_3h', \n",
    "                'precip',\n",
    "                'clouds_all',\n",
    "                'temp_max',\n",
    "                'snow_3h']\n",
    "\n",
    "df = df[vars_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       day|count|\n",
      "+----------+-----+\n",
      "|2020-02-26|44503|\n",
      "|2019-08-08|68714|\n",
      "+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_df = df.select(F.date_format('date','yyyy-MM-dd').alias('day')).groupby('day').count()\n",
    "count_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+--------+--------+----------+-------+---------+----------+--------+-------+\n",
      "|      date|  temp|feels_like|pressure|humidity|wind_speed|rain_3h|   precip|clouds_all|temp_max|snow_3h|\n",
      "+----------+------+----------+--------+--------+----------+-------+---------+----------+--------+-------+\n",
      "|2019-08-08|70.664|    76.748|    1009|     100|       1.5|    0.0|   precip|        90|  69.008|    0.0|\n",
      "|2019-08-22|77.972|    87.224|    1011|      94|      1.39|    0.0|no_precip|        75|  75.002|    0.0|\n",
      "+----------+------+----------+--------+--------+----------+-------+---------+----------+--------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.dropDuplicates(['date'])\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.join(count_df,df.date ==  count_df.day,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+--------+--------+----------+-------+---------+----------+--------+-------+----------+-----+\n",
      "|      date|  temp|feels_like|pressure|humidity|wind_speed|rain_3h|   precip|clouds_all|temp_max|snow_3h|       day|count|\n",
      "+----------+------+----------+--------+--------+----------+-------+---------+----------+--------+-------+----------+-----+\n",
      "|2019-08-08|70.664|    76.748|    1009|     100|       1.5|    0.0|   precip|        90|  69.008|    0.0|2019-08-08|68714|\n",
      "|2019-08-22|77.972|    87.224|    1011|      94|      1.39|    0.0|no_precip|        75|  75.002|    0.0|2019-08-22|60527|\n",
      "|2019-08-23|81.842|    87.926|    1011|      69|       1.5|    0.0|no_precip|        20|  78.998|    0.0|2019-08-23|54493|\n",
      "|2020-02-26|46.256|    42.296|    1011|      93|       2.1|    0.0|   precip|        90|  44.006|    0.0|2020-02-26|44503|\n",
      "+----------+------+----------+--------+--------+----------+-------+---------+----------+--------+-------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a count of station\n",
    "grouped_data = df1.groupby(\"count\")\n",
    "agg_data = grouped_data.agg(F.avg(\"feels_like\").alias(\"avg_feels_like\"),\n",
    "                 F.avg(\"wind_speed\").alias(\"avg_wind_speed\"),\n",
    "                 F.avg(\"temp_max\").alias(\"avg_temp_max\"),\n",
    "                 F.avg(\"temp\").alias(\"avg_temp\"),\n",
    "                 F.avg(\"pressure\").alias(\"avg_pressure\"),\n",
    "                 F.avg(\"humidity\").alias(\"avg_humidity\"),\n",
    "                 F.avg(\"rain_3h\").alias(\"avg_rain_3h\"),\n",
    "                 F.avg(\"snow_3h\").alias(\"avg_snow_3h\"),\n",
    "                 F.count(when(df.clouds_all == 1, lit(1))).alias(\"clouds_count\"),\n",
    "                 F.count(when(df.clouds_all == 0, lit(1))).alias(\"no_clouds_count\"),\n",
    "                 F.count(when(df.precip == \"precip\", lit(1))).alias(\"precip_count\"),\n",
    "                 F.count(when(df.precip == \"no_precip\", lit(1))).alias(\"non_precip_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------+------------+--------+------------+------------+-----------+-----------+------------+---------------+------------+----------------+\n",
      "|count|avg_feels_like|avg_wind_speed|avg_temp_max|avg_temp|avg_pressure|avg_humidity|avg_rain_3h|avg_snow_3h|clouds_count|no_clouds_count|precip_count|non_precip_count|\n",
      "+-----+--------------+--------------+------------+--------+------------+------------+-----------+-----------+------------+---------------+------------+----------------+\n",
      "|53294|        36.716|          4.63|        46.4|  48.506|      1021.0|        18.0|        0.0|        0.0|           1|              0|           0|               1|\n",
      "|47928|        89.186|          1.54|      80.443|  84.956|      1018.0|        55.0|        0.0|        0.0|           1|              0|           0|               1|\n",
      "|31762|        52.196|          2.36|       53.06|   56.93|      1003.0|        58.0|        0.0|        0.0|           1|              0|           0|               1|\n",
      "+-----+--------------+--------------+------------+--------+------------+------------+-----------+-----------+------------+---------------+------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agg_data = agg_data.join(count_df,agg_data.date ==  count_df.day,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agg_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_names = agg_data.columns\n",
    "#feat = agg_data.rdd.map(lambda row: row[0:])\n",
    "#Corr\n",
    "#corr_ = stat.corr(feat, method=\"pearson\")\n",
    "#Corr to pandas DF\n",
    "#corr_df = pd.DataFrame(corr_)\n",
    "#corr_df.index, corr_df.columns = col_names, col_names\n",
    "#corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count',\n",
       " 'avg_feels_like',\n",
       " 'avg_wind_speed',\n",
       " 'avg_temp_max',\n",
       " 'avg_temp',\n",
       " 'avg_pressure',\n",
       " 'avg_humidity',\n",
       " 'avg_rain_3h',\n",
       " 'avg_snow_3h',\n",
       " 'clouds_count',\n",
       " 'no_clouds_count',\n",
       " 'precip_count',\n",
       " 'non_precip_count']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums_feat = [i[0] for i in agg_data.dtypes if i[1] != 'string']\n",
    "nums_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PIPELINE indexer-standard scalar\n",
    "#indexer = StringIndexer(inputCol=\"zipcodes\", outputCol=\"zips_indexed\")\n",
    "\n",
    "#encoder = OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=\"zips_encoded\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=list((set(nums_feat)-set([\"count\"]))), outputCol=\"vectorized_features\")\n",
    "\n",
    "standardScaler = StandardScaler(inputCol=assembler.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "#pipeline\n",
    "pipeline = Pipeline(stages=[assembler] + [standardScaler])\n",
    "model = pipeline.fit(agg_data)\n",
    "df_tr = model.transform(agg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_tr.randomSplit([0.7,0.3], seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(labelCol='count', featuresCol='features', maxIter=3)\n",
    "lrModel = lr.fit(train)\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-1177.6794802513764,0.0,176.94168284522559,1908.147964971849,4305.624231654213,0.0,-823.3042687866946,4773.6443021584055,837.8466822266832,4127.860098986759,0.0,-26.96217197878529]\n",
      "Intercept: -234287.11963154475\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 15472.009610796167\n",
      "r2: 0.4074282440368653\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: \" + str(trainingSummary.rootMeanSquaredError)) \n",
    "print(\"r2: \" + str(trainingSummary.r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70139.238945</td>\n",
       "      <td>47928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35384.569772</td>\n",
       "      <td>53294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44586.918142</td>\n",
       "      <td>61577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54732.689653</td>\n",
       "      <td>88979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51599.519033</td>\n",
       "      <td>57177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction  count\n",
       "0  70139.238945  47928\n",
       "1  35384.569772  53294\n",
       "2  44586.918142  61577\n",
       "3  54732.689653  88979\n",
       "4  51599.519033  57177"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"prediction\",\"count\").toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change response varaible to 'label'\n",
    "df_tr_2 = df_tr.withColumnRenamed('count', 'label')\n",
    "#Train/test split\n",
    "train, test = df_tr_2.randomSplit([0.7,0.3], seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune with CV - Base ModelLinear Regression tuned \n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "\n",
    "\n",
    "#Linear regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "#Paramgrid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0]) #regularization param\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) #Ridge=0, lasso=1.0\n",
    "             .addGrid(lr.maxIter, [1, 5, 10]) #maxIter\n",
    "             .build())\n",
    "\n",
    "\n",
    "\n",
    "#10 fold cross validation\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps = paramGrid, evaluator=RegressionEvaluator(), numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_preds = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating metrics\n",
    "eval_rmse = RegressionEvaluator(metricName=\"rmse\")\n",
    "eval_r2 = RegressionEvaluator(metricName=\"r2\")\n",
    "best_rmse = eval_rmse.evaluate(tune_preds)\n",
    "best_r2 = eval_r2.evaluate(tune_preds)\n",
    "print(\"R Squared (R2) on test data : \" + str(best_r2)) #0.53685\n",
    "print(\"RMSE on test data : \" + str(best_rmse)) #107388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best model paramaters\n",
    "bm = cvModel.bestModel\n",
    "\n",
    "print(\"Best model regularlization parameter: \", bm._java_obj.getRegParam())\n",
    "print(\"Best model elastic net parameter: \", bm._java_obj.getElasticNetParam()) #lasso\n",
    "print(\"Best model iteration: \", bm._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Feature weights\n",
    "best_coef = bm.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of importances and sort and take top 10\n",
    "lis = []\n",
    "for i in best_coef:\n",
    "    lis.append(i)\n",
    "output = list(zip(lis, nums_feat[1:]))\n",
    "output = sorted(output, key=lambda x: x[0], reverse=True)\n",
    "#The weights were numpy-float64 so I needed to change its type to be able to create spark df\n",
    "output = [(float(x[0]), x[1]) for x in output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get DF\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "\n",
    "\n",
    "#get DF\n",
    "schema = StructType([\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "df_ticks = spark.createDataFrame(output, schema).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect\n",
    "df_ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
