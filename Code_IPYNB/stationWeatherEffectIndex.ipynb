{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a station good and bad weather index\n",
    "\n",
    "Attempt to create an index of the resiliance of a CitiBike station use to seasonally adjusted changes in weather.\n",
    "\n",
    "* weatherIndex of 1.0 means the station has increased use on monthyly-adjusted good weather days vs. monthyly-adjusted bad weather days.\n",
    "* weatherIndex of 0.0 means that station has identical use on monthyly-adjusted good weather days vs. monthyly-adjusted bad weather days.\n",
    "* weatherIndex of -1.0 means that the station has decreased use on monthyly-adjusted good weather days vs. monthyly-adjusted bad weather days.\n",
    "\n",
    "The weatherIndex is of use when characterizing a station for business purposes, and grouping CitiBike stations by similar use.\n",
    "\n",
    "\n",
    "### Calculation\n",
    "1. Need to categorize each day by good or bad monthly-adjusted weather based on feels_like, humidity, wind_speed, precip\n",
    "    1. get mean and average feels_like for month.\n",
    "    1. get mean and average humidity for month.\n",
    "    1. get mean and average wind_speed per month.\n",
    "    1. Curious about count of precip days per month.\n",
    "\n",
    "\n",
    "# ALWAYS RUN THIS NEXT CELL\n",
    "with the import libraries and spark context and session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from pyspark.sql.functions import count, avg, stddev, col, udf\n",
    "from pyspark.sql.functions import lit, concat\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "# import modules\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import os\n",
    "# start spark context\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"weatherwork\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# set configurations\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"weatherwork\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Write example code for brining in the good/bad by date broadcast variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/hdm5s/ds5110/FinalProject/gt2017_stationName_withGoodBadWeather.parquet\n",
    "goodBadByDate_filepath = \"/home/hdm5s/ds5110/FinalProject/broadcastGoodBadWeatherByDate.csv\"\n",
    "sdf_goodBadByDate = spark.read.csv(goodBadByDate_filepath).rdd.collectAsMap()\n",
    "bc_goodbadweatherdate = sc.broadcast(sdf_goodBadByDate)\n",
    "\n",
    "# to access the GOOD OR BAD weather use tripdate\n",
    "# bc_goodbadweatherdate.value.get(tripdate)\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def isBadOrGoodWeather(tripdate : str):\n",
    "    return bc_goodbadweatherdate.value.get(tripdate)\n",
    "\n",
    "\n",
    "# load the final data file with only the columns I am interested in using:\n",
    "file = \"/project/ds5559/Summer2021_TeamBike/gt2017_bikeweather.parquet\" # FINAL DATA FILE\n",
    "sdf = spark.read.parquet(file).select('date', 'tripduration', 'startStationId', 'startStationName', 'startStationLatitude','startStationLongitude','endStationId', \n",
    "                                      'endStationName', 'endStationLatitude', 'endStationLongitude', 'usertype', 'feels_like', 'humidity', 'wind_speed', 'weather_main',\n",
    "                                      'dow', 'day', 'month', 'time_bin', 'peak_commute', 'year', 'precip')\n",
    "sdf = sdf.withColumn(\"weatherGoodOrBad\", isBadOrGoodWeather(col(\"date\")))\n",
    "\n",
    "# now you have the weatherGoodOrBad column for every record with either GOOD, or BAD values in it. This is the monthly adjusted binary good/bad weather category\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Write example code for bringing in the stationGoodWeatherIndex and stationBadWeatherIndex\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE CELLS until the 'RUN NOW' cell \n",
    "These are here for documentation of the process to create the low and high feels-like monthly broadcast variable csv's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the standard data file with only the columns I am interested in using:\n",
    "# 'startStationLatitude', 'startStationLongitude','tripduration','month','year'\n",
    "file = \"/project/ds5559/Summer2021_TeamBike/gt2017_bikeweather.parquet\" # FINAL DATA FILE\n",
    "sdf = spark.read.parquet(file).select('date', 'hour', 'tripduration', 'starttime', 'startStationId', 'startStationLatitude', 'startStationLongitude',\n",
    "    'endStationId', 'endStationName', 'endStationLatitude', 'endStationLongitude', 'bikeid', 'usertype', 'birthyear', 'feels_like', 'humidity', 'wind_speed',\n",
    "    'weather_main', 'dow', 'day', 'time_bin', 'peak_commute', 'precip','month','year')\n",
    "sdf.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by month, year and get the average tripduration, and the count of items in the aggregation\n",
    "sdf_grouped = sdf.groupBy('month').agg(\n",
    "    avg('feels_like').alias('avg_feels_like'), \n",
    "    stddev('feels_like').alias('std_feels_like'),\n",
    "    avg('humidity').alias('avg_humidity'), \n",
    "    stddev('humidity').alias('std_humidity'),\n",
    "    avg('wind_speed').alias('avg_wind_speed'), \n",
    "    stddev('wind_speed').alias('std_wind_speed'),\n",
    "    stddev('wind_speed').alias('std_wind_speed'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_grouped.show(12)\n",
    "sdf_grouped.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by month, year and get the average tripduration, and the count of items in the aggregation\n",
    "sdf_precip_grouped = sdf.groupBy('month', 'precip').agg(\n",
    "    count('date').alias('precip_count')\n",
    ")\n",
    "sdf_precip_grouped.orderBy('month','precip').show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Makes a Bad Weather Day?\n",
    "\n",
    "#### Wind Speed and Precip are relatively easy to determine threshold for good vs. bad weather day:\n",
    "1. if **wind_speed >= 10 == BAD weather day.** \n",
    "    1. wind speed isn't changing much month-to-month. And somewhere between 10 and 15 mph is recogunized to be a windy by cyclists based on a informal survey of five urban bicycle commuters.\n",
    "1. 1. precip == precip == BAD weather day.\n",
    "    1. For our purposes, if a day has precipiation we are calling it a \"BAD\" weather day.\n",
    "    \n",
    "    \n",
    "    \n",
    "#### What About Feels Like?\n",
    "feels_like is harder because a seasonally adjusted feels_like \"bad\" weather day in January is very different from a seasonally-adjusted \"bad\" weather day in August.\n",
    "\n",
    "Based on the mean and std by month for feels_like I propose the following:\n",
    "\n",
    "| Month       | BAD condition LOW   |  BAD condition HIGH\n",
    "| :------------- | :----------: | :----------: |\n",
    "| Jan | < (mean - 1 * std) | 90 |\n",
    "| Feb | < (mean - 1 * std  | 90 |\n",
    "| Mar | < (mean - 1 * std) | 90 |\n",
    "| Apr | < (mean - 1 * std) | 90 |\n",
    "| May | < (mean - 2 * std) | 90 |\n",
    "| Jun | 45 | > (mean + 1.5 * std) |\n",
    "| Jul | 45 | > (mean + 1.5 * std) |\n",
    "| Aug | 45 | > (mean + 1.5 * std) |\n",
    "| Sep | < (mean - 3 * std)  | 90 |\n",
    "| Oct | < (mean - 1 * std) | 90 |\n",
    "| Nov | < (mean - 1 * std) | 90 |\n",
    "| Dec | < (mean - 1 * std) | 90 |\n",
    "\n",
    "#### What About Humidity?\n",
    "Based on the relationship between humidity, precip and feels_like, and the relative stability of humidity I propose we do not include humidity in our BAD weather classification.\n",
    "\n",
    "### Next Step: create the broadcast variables for feels_like_LOW and feels_like_HIGH for each month\n",
    "https://sparkbyexamples.com/pyspark/pyspark-broadcast-variables/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why doesn't python have a switch case statment !!???!!!\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def calcFeelsLikeLow(fl_month, fl_avg, fl_std):\n",
    "    # default value\n",
    "    feels_like_low = 45\n",
    "    \n",
    "    if fl_month == \"Jan\" or fl_month == \"Feb\" or fl_month == \"Mar\" or fl_month == \"Apr\" or fl_month == \"Oct\" or fl_month == \"Nov\" or fl_month == \"Dec\" or fl_month == \"May\":\n",
    "        feels_like_low = fl_avg - (fl_std)\n",
    "    elif fl_month == \"Sep\":\n",
    "        feels_like_low = fl_avg - (2 * fl_std)\n",
    "    else:\n",
    "        feels_like_low = 45.0\n",
    "    \n",
    "    return feels_like_low\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def calcFeelsLikeHigh(fl_month, fl_avg, fl_std):\n",
    "    # default value\n",
    "    feels_like_high = 90.0\n",
    "    \n",
    "    if fl_month == \"Jul\" or fl_month == \"Aug\" or fl_month == \"Jun\":\n",
    "        feels_like_high = fl_avg + (1.5 * fl_std)\n",
    "    else:\n",
    "        feels_like_high = 90.0\n",
    "    \n",
    "    return feels_like_high\n",
    "\n",
    "\n",
    "sdf_grouped = sdf_grouped.withColumn(\"feels_like_LOW\", calcFeelsLikeLow(col(\"month\"), col(\"avg_feels_like\"), col(\"std_feels_like\"))).withColumn(\"feels_like_HIGH\", calcFeelsLikeHigh(col(\"month\"), col(\"avg_feels_like\"), col(\"std_feels_like\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_grouped.show()\n",
    "sdf_grouped.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate bad weather across the entire data set for each day in the dataset based on critia:\n",
    "1. if feels_like < feels_like_LOW for the month\n",
    "1. if feels like > feels_like_HIGH for the month\n",
    "1. if precip == precip\n",
    "\n",
    "\n",
    "# NEXT TASKS\n",
    "1. Assign Good or Bad weather to every ride in the dataset\n",
    "Create the Broadcast Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a broadcast of month, feels_like_LOW and month, feels_like_HIGH\n",
    "sdf_month_low = sdf_grouped.rdd.map(lambda x: (x[0], x[8])).collectAsMap()                                                       \n",
    "monthLowBroadcast = sc.broadcast(sdf_month_low)\n",
    "\n",
    "\n",
    "sdf_month_high = sdf_grouped.rdd.map(lambda x: (x[0], x[9])).collectAsMap()                                                       \n",
    "monthHighBroadcast = sc.broadcast(sdf_month_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the low broadcast to csv\n",
    "sdf_grouped.rdd.map(lambda x: (x[0], x[8])).toDF().write.csv('/home/hdm5s/ds5110/FinalProject/broadcastLow.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Broadbase variables to create the BAD weather indicator for each row in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the high broadcast to csv\n",
    "sdf_grouped.rdd.map(lambda x: (x[0], x[9])).toDF().write.csv('/home/hdm5s/ds5110/FinalProject/broadcastHigh.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN NOW: IF you want to ...\n",
    "create the gt2017- data file that includes the weatherGoodOrBad column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the standard data file with only the columns I am interested in using:\n",
    "file = \"/project/ds5559/Summer2021_TeamBike/gt2017_bikeweather.parquet\" # FINAL DATA FILE\n",
    "sdf = spark.read.parquet(file).select('date', 'startStationId', 'startStationName', 'feels_like', 'month','year','precip')\n",
    "sdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the low and high rdds\n",
    "rdd_low = spark.read.csv(\"/home/hdm5s/ds5110/FinalProject/broadcastLow.csv\", header=False).rdd.collectAsMap()\n",
    "monthLowBroadcast = sc.broadcast(rdd_low)\n",
    "\n",
    "rdd_high = spark.read.csv(\"/home/hdm5s/ds5110/FinalProject/broadcastHigh.csv\", header=False).rdd.collectAsMap()\n",
    "monthHighBroadcast = sc.broadcast(rdd_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthLowBroadcast.value.get('Oct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthHighBroadcast.value.get('Oct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the broadcasts for low and high and comparing with the feels_like\n",
    "@udf(returnType=StringType())\n",
    "def isBadOrGoodWeather(month, feels_like, precip):\n",
    "    goodOrBad = \"GOOD\"\n",
    "    \n",
    "    if float(feels_like) <= float(monthLowBroadcast.value.get(month)) or float(feels_like) >= float(monthHighBroadcast.value.get(month)) or precip == \"precip\":\n",
    "        goodOrBad = \"BAD\"\n",
    "    else:\n",
    "        goodOrBad = \"GOOD\"\n",
    "        \n",
    "    return goodOrBad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"weatherGoodOrBad\", isBadOrGoodWeather(col(\"month\"), col(\"feels_like\"), col(\"precip\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe with Good/Bad weather columns\n",
    "sdf.write.parquet('/home/hdm5s/ds5110/FinalProject/gt2017_stationName_withGoodBadWeather.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by month, year and get the average tripduration, and the count of items in the aggregation\n",
    "sdf.groupBy('month', 'weatherGoodOrBad').count().show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a nice distribution of good and bad weather days.\n",
    "\n",
    "**Save a csv broadcast of date** \n",
    "1. Create a saved broadcast of date and GOOD or BAD weatherGoodOrBad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the low broadcast to csv\n",
    "#sdf.rdd.map(lambda x: (x[0], x[7])).toDF().write.csv('/home/hdm5s/ds5110/FinalProject/broadcastGoodBadWeatherByDate.csv')\n",
    "# [0] == date\n",
    "# [7] == weatherGOODOrBad\n",
    "sdf.rdd.map(lambda x: (x[0], x[7])).toDF().write.format('csv').option('header',False).mode('overwrite').save('/home/hdm5s/ds5110/FinalProject/broadcastGoodBadWeatherByDate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN THIS if you want to ...\n",
    "create the good weather and bad weather index by startStationName broadcast variable\n",
    "1. Group by start station: Good weather station Index: good weather rides day average count rides / average day average count rides (EXPECT > 1, but HOW MUCH greater than 1)\n",
    "1. Group by start station: Bad weather station Index:  bad weather ride day average count rides / average day average count rides (EXPECT < 1, but HOW MUCH less than 1)\n",
    "1. Create a saved broadcast csv with the stationName goodWeatherIndex\n",
    "1. Create a saved broadcast csv with stationName badWeatherIndex\n",
    "\n",
    "# BUT SKIP UNTIL 'RUN NOW' IF YOU JUST WANT TO LOAD THE ALREADY CREATED broadcast variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/hdm5s/ds5110/FinalProject/gt2017_stationName_withGoodBadWeather.parquet\n",
    "goodBadByDate_filepath = \"/home/hdm5s/ds5110/FinalProject/broadcastGoodBadWeatherByDate.csv\"\n",
    "sdf_goodBadByDate = spark.read.csv(goodBadByDate_filepath).rdd.collectAsMap()\n",
    "bc_goodbadweatherdate = sc.broadcast(sdf_goodBadByDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access the GOOD OR BAD weather use tripdate\n",
    "# bc_goodbadweatherdate.value.get(tripdate)\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def isBadOrGoodWeather(tripdate : str):\n",
    "    return bc_goodbadweatherdate.value.get(tripdate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the standard data file with only the columns I am interested in using:\n",
    "file = \"/project/ds5559/Summer2021_TeamBike/gt2017_bikeweather.parquet\" # FINAL DATA FILE\n",
    "sdf = spark.read.parquet(file).select('date', 'tripduration', 'startStationId', 'startStationName', 'startStationLatitude','startStationLongitude','endStationId', \n",
    "                                      'endStationName', 'endStationLatitude', 'endStationLongitude', 'usertype', 'feels_like', 'humidity', 'wind_speed', 'weather_main',\n",
    "                                      'dow', 'month', 'time_bin', 'peak_commute', 'year', 'precip')\n",
    "sdf = sdf.withColumn(\"weatherGoodOrBad\", isBadOrGoodWeather(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+--------------+----------------+--------------------+---------------------+------------+--------------+------------------+-------------------+--------+----------+--------+----------+------------+---+-----+--------+------------+----+------+----------------+\n",
      "|date|tripduration|startStationId|startStationName|startStationLatitude|startStationLongitude|endStationId|endStationName|endStationLatitude|endStationLongitude|usertype|feels_like|humidity|wind_speed|weather_main|dow|month|time_bin|peak_commute|year|precip|weatherGoodOrBad|\n",
      "+----+------------+--------------+----------------+--------------------+---------------------+------------+--------------+------------------+-------------------+--------+----------+--------+----------+------------+---+-----+--------+------------+----+------+----------------+\n",
      "+----+------------+--------------+----------------+--------------------+---------------------+------------+--------------+------------------+-------------------+--------+----------+--------+----------+------------+---+-----+--------+------------+----+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# are there nulls in weatherGoodOrBad?\n",
    "sdf.filter(col(\"weatherGoodOrBad\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+--------------------+--------------------+---------------------+------------+------------------+------------------+-------------------+----------+----------+--------+----------+------------+---+-----+--------+------------+----+---------+----------------+\n",
      "|      date|tripduration|startStationId|    startStationName|startStationLatitude|startStationLongitude|endStationId|    endStationName|endStationLatitude|endStationLongitude|  usertype|feels_like|humidity|wind_speed|weather_main|dow|month|time_bin|peak_commute|year|   precip|weatherGoodOrBad|\n",
      "+----------+------------+--------------+--------------------+--------------------+---------------------+------------+------------------+------------------+-------------------+----------+----------+--------+----------+------------+---+-----+--------+------------+----+---------+----------------+\n",
      "|2018-10-01|         330|         293.0|Lafayette St & E ...|   40.73020660529954|   -73.99102628231049|       504.0|   1 Ave & E 16 St|       40.73221853|       -73.98165557|Subscriber|    64.508|      77|      2.65|       Clear|  2|  Oct|   Night|    non_peak|2018|no_precip|            GOOD|\n",
      "|2018-10-01|         309|         433.0|old - E 13 St & A...|         40.72955361|         -73.98057249|       394.0| E 9 St & Avenue C|       40.72521311|       -73.97768752|Subscriber|    64.508|      77|      2.65|       Clear|  2|  Oct|   Night|    non_peak|2018|no_precip|            GOOD|\n",
      "|2018-10-01|         734|        3472.0|    W 15 St & 10 Ave|  40.742753828659026|   -74.00747358798982|       285.0|Broadway & E 14 St|       40.73454567|       -73.99074142|Subscriber|    64.508|      77|      2.65|       Clear|  2|  Oct|   Night|    non_peak|2018|no_precip|            GOOD|\n",
      "|2018-10-01|         788|         417.0|Barclay St & Chur...|         40.71291224|         -74.01020234|      3472.0|  W 15 St & 10 Ave|40.742753828659026| -74.00747358798982|  Customer|    64.508|      77|      2.65|       Clear|  2|  Oct|   Night|    non_peak|2018|no_precip|            GOOD|\n",
      "|2018-10-01|        1136|        3699.0|     W 50 St & 9 Ave|   40.76360467795863|   -73.98917958140373|       495.0|  W 47 St & 10 Ave|       40.76269882|       -73.99301222|Subscriber|    64.508|      77|      2.65|       Clear|  2|  Oct|   Night|    non_peak|2018|no_precip|            GOOD|\n",
      "+----------+------------+--------------+--------------------+--------------------+---------------------+------------+------------------+------------------+-------------------+----------+----------+--------+----------+------------+---+-----+--------+------------+----+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, tripduration: bigint, startStationId: string, startStationName: string, startStationLatitude: double, startStationLongitude: double, endStationId: double, endStationName: string, endStationLatitude: double, endStationLongitude: double, usertype: string, feels_like: double, humidity: bigint, wind_speed: double, weather_main: string, dow: int, month: string, time_bin: string, peak_commute: string, year: int, precip: string, weatherGoodOrBad: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next**, group by station and month and view the following:\n",
    "* average trip count per month\n",
    "* average trip count when weather good per month\n",
    "* average trip count when weather bad per month\n",
    "* average duration per month\n",
    "* average duration when weather good per month\n",
    "* average duration when weather bad per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by month, startStationName and get the average tripduration, and the count of items in the aggregation\n",
    "sdf_month_station = sdf.groupBy('month', 'startStationName').agg(\n",
    "    count('tripduration').alias('counttrips'), \n",
    "    avg('tripduration').alias('avg_tripduration')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+------------------+\n",
      "|month|    startStationName|counttrips|  avg_tripduration|\n",
      "+-----+--------------------+----------+------------------+\n",
      "|  Oct|  W 67 St & Broadway|     23694| 866.0102557609522|\n",
      "|  Oct|Cooper Square & A...|     41372| 713.8334622449967|\n",
      "|  Oct|Hanson Pl & Ashla...|     21984| 1386.932996724891|\n",
      "|  Oct|     E 58 St & 3 Ave|     25413|   891.34734191162|\n",
      "|  Oct|  E 98 St & Park Ave|      2383| 830.2194712547209|\n",
      "|  Nov|Allen St & Riving...|     19748| 847.1000607656472|\n",
      "|  Nov|    W 12 St & W 4 St|      2979|  713.860355824102|\n",
      "|  Nov|E 110 St & Madiso...|      5953| 1119.487149336469|\n",
      "|  Nov|  E 98 St & Park Ave|      2026| 734.2290227048371|\n",
      "|  Dec|   Kent Ave & N 7 St|      8919|1102.0799416974996|\n",
      "|  Dec|Pioneer St & Rich...|       661| 772.3373676248109|\n",
      "|  Dec|Howard St & Centr...|     10941| 769.8045882460469|\n",
      "|  Dec|Adam Clayton Powe...|      1808|  1340.64657079646|\n",
      "|  Dec| Henry St & Grand St|     12724| 780.9906475950959|\n",
      "|  Jun|W 45 St & 6 Ave (...|      9074| 871.6200132245978|\n",
      "|  May|E 82 St & East En...|      8201|1153.9542738690404|\n",
      "|  May|     W 13 St & 7 Ave|     29218| 877.1151345061263|\n",
      "|  May|       48 Ave & 5 St|      4663|1104.6680248766888|\n",
      "|  May|Greenwich Ave & C...|     33782|1184.9995559765555|\n",
      "|  May|      28 Ave & 44 St|      1522|1246.4415243101182|\n",
      "|  May|Pacific St & Clas...|      1941|1190.3910355486862|\n",
      "|  May|    W 15 St & 10 Ave|     20221| 1457.880520251224|\n",
      "|  Jul|Railroad Ave & Ka...|      1022| 884.2592954990215|\n",
      "|  Jul|     W 27 St & 7 Ave|     35120| 821.2402619589977|\n",
      "|  May|2 Ave & 36 St - C...|      2528|2470.2104430379745|\n",
      "+-----+--------------------+----------+------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_month_station.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_month_station.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a broadcast of month-station-countrips\n",
    "sdf_month_station_trips = sdf_month_station.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]), x[2])).collectAsMap()                                                       \n",
    "monthStationTrips = sc.broadcast(sdf_month_station_trips)\n",
    "\n",
    "# create a broadcast of month-station-avgDuration\n",
    "sdf_month_station_duration = sdf_month_station.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]), x[3])).collectAsMap()                                                       \n",
    "monthStationDuration = sc.broadcast(sdf_month_station_duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the broadcasts to file\n",
    "sdf_month_station.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]), x[2])).toDF().write.mode(\"overwrite\").csv('/home/hdm5s/ds5110/FinalProject/broadcastMonthStationTrips.csv')\n",
    "sdf_month_station.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]), x[3])).toDF().write.mode(\"overwrite\").csv('/home/hdm5s/ds5110/FinalProject/broadcastMonthStationDuration.csv')\n",
    "\n",
    "sdf_month_station.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]), x[2])).toDF().write.mode(\"overwrite\").csv('/project/ds5559/Summer2021_TeamBike/broadcastMonthStationTrips.csv')\n",
    "sdf_month_station.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]), x[3])).toDF().write.mode(\"overwrite\").csv('/project/ds5559/Summer2021_TeamBike/broadcastMonthStationDuration.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by month, startStationName, and weatherGoodOrBad and get the average tripduration, and the count of items in the aggregation\n",
    "sdf_month_station_goodBad = sdf.groupBy('month', 'startStationName', 'weatherGoodOrBad').agg(\n",
    "    count('tripduration').alias('counttrips'), \n",
    "    avg('tripduration').alias('avg_tripduration')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------------+----------+------------------+\n",
      "|month|    startStationName|weatherGoodOrBad|counttrips|  avg_tripduration|\n",
      "+-----+--------------------+----------------+----------+------------------+\n",
      "|  Oct|Lafayette St & E ...|            GOOD|     34681| 732.5926876387647|\n",
      "|  Oct|Central Park West...|            GOOD|     16946| 1294.232916322436|\n",
      "|  Oct|        7 St & 3 Ave|            GOOD|      2628| 834.7534246575342|\n",
      "|  Oct|    W 26 St & 10 Ave|            GOOD|     12055| 909.6550808793032|\n",
      "|  Oct|  Broadway & W 51 St|            GOOD|     14936| 918.5858998393144|\n",
      "|  Oct|      23 Ave & 27 St|            GOOD|      1252| 986.8825878594249|\n",
      "|  Oct|Carroll St & Fran...|            GOOD|       999| 1135.003003003003|\n",
      "|  Oct|    W 47 St & 10 Ave|             BAD|      5558| 835.7563871896366|\n",
      "|  Nov|E 24 St & Park Ave S|            GOOD|     17351| 671.8246786928707|\n",
      "|  Nov|S 3 St & Bedford Ave|            GOOD|      7084|1123.1077075098815|\n",
      "|  Nov|Park Pl & Frankli...|            GOOD|      2004| 929.8662674650699|\n",
      "|  Nov|Kane St & Clinton St|            GOOD|      3950| 766.9473417721518|\n",
      "|  Nov|Coffey St & Conov...|            GOOD|      1525|1089.8373770491803|\n",
      "|  Nov|       40 Ave & 9 St|            GOOD|       724|1018.0662983425415|\n",
      "|  Nov|Fulton St & Broadway|             BAD|      3622| 809.0993926007731|\n",
      "|  Nov|Perry St & Bleeck...|             BAD|      2247| 710.9572763684913|\n",
      "|  Nov|India St & Manhat...|             BAD|      1354| 688.4852289512555|\n",
      "|  Nov|Front St & Maiden Ln|             BAD|      1673| 953.8242677824268|\n",
      "|  Nov|Franklin St & Dup...|             BAD|      1123| 743.5316117542297|\n",
      "|  Dec|Barclay St & Chur...|            GOOD|      7396| 834.6292590589508|\n",
      "|  Dec|Central Park West...|             BAD|       803|1205.7708592777085|\n",
      "|  Dec|Prospect Pl & Und...|             BAD|       606| 828.6914191419141|\n",
      "|  Dec|President St & He...|             BAD|       672| 615.9315476190476|\n",
      "|  Dec| Degraw St & Hoyt St|             BAD|       619| 604.9321486268175|\n",
      "|  Jun|Schermerhorn St &...|            GOOD|      8218|1091.4050863957168|\n",
      "+-----+--------------------+----------------+----------+------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_month_station_goodBad.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_month_station_goodBad.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a broadcast of month-station-weather-countrips\n",
    "sdf_month_station_goodbad_trips = sdf_month_station_goodBad.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]) + str(x[2]), x[3])).collectAsMap()                                                       \n",
    "monthStationGoodBadTrips = sc.broadcast(sdf_month_station_goodbad_trips)\n",
    "\n",
    "# create a broadcast of month-station-weather-avgDuration\n",
    "sdf_month_station_goodbad_duration = sdf_month_station_goodBad.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]) + str(x[2]), x[4])).collectAsMap()                                                       \n",
    "monthStationGppdBadDuration = sc.broadcast(sdf_month_station_goodbad_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the broadcasts to file\n",
    "sdf_month_station_goodBad.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]) + str(x[2]), x[3])).toDF().write.mode(\"overwrite\").csv('/home/hdm5s/ds5110/FinalProject/broadcastMonthStationWeatherGoodBadTrips.csv')\n",
    "sdf_month_station_goodBad.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]) + str(x[2]), x[4])).toDF().write.mode(\"overwrite\").csv('/home/hdm5s/ds5110/FinalProject/broadcastMonthStationWeatherGoodBadDuration.csv')\n",
    "\n",
    "sdf_month_station_goodBad.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]) + str(x[2]), x[3])).toDF().write.mode(\"overwrite\").csv('/project/ds5559/Summer2021_TeamBike/broadcastMonthStationWeatherGoodBadTrips.csv')\n",
    "sdf_month_station_goodBad.rdd.map(lambda x: (str(x[0]) + \"-\" + str(x[1]) + str(x[2]), x[4])).toDF().write.mode(\"overwrite\").csv('/project/ds5559/Summer2021_TeamBike/broadcastMonthStationWeatherGoodBadDuration.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN NOW, to ... \n",
    "apply the good/bad index for the month and station for all trips in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key Oct-Maiden Ln & Pearl St, value 12743 \n",
      "key Oct-E 81 St & York Ave, value 11707 \n",
      "key Oct-W 16 St & The High Line, value 24878 \n",
      "key Oct-Maiden Ln & Pearl St, value 1050.0648199011223 \n",
      "key Oct-E 81 St & York Ave, value 873.0382677030836 \n",
      "key Oct-W 16 St & The High Line, value 823.0103304124126 \n",
      "key Oct-Washington Ave & Greene AveGOOD, value 4450 \n",
      "key Oct-E 7 St & Avenue ABAD, value 9344 \n",
      "key Oct-Carlton Ave & Dean StBAD, value 1346 \n",
      "key Oct-Washington Ave & Greene AveGOOD, value 837.4429213483146 \n",
      "key Oct-E 7 St & Avenue ABAD, value 688.5225813356165 \n",
      "key Oct-Carlton Ave & Dean StBAD, value 737.2303120356612 \n"
     ]
    }
   ],
   "source": [
    "# load the broadcast variables from csv\n",
    "# trips by station and month\n",
    "tripsByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationTrips.csv\"\n",
    "sdf_tripsByStationMonth = spark.read.csv(tripsByStationMonth_filepath).rdd.collectAsMap()\n",
    "for x in list(sdf_tripsByStationMonth)[0:3]:\n",
    "    print (\"key {}, value {} \".format(x,  sdf_tripsByStationMonth[x]))\n",
    "bc_tripsByStationMonth = sc.broadcast(sdf_tripsByStationMonth)\n",
    "\n",
    "# duration by station and month\n",
    "durationByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationDuration.csv\"\n",
    "sdf_durationByStationMonth = spark.read.csv(durationByStationMonth_filepath).rdd.collectAsMap()\n",
    "for x in list(sdf_durationByStationMonth)[0:3]:\n",
    "    print (\"key {}, value {} \".format(x,  sdf_durationByStationMonth[x]))\n",
    "bc_durationByStationMonth = sc.broadcast(sdf_durationByStationMonth)\n",
    "\n",
    "\n",
    "# trips by station and month and good/bad weather\n",
    "goodBadTripsByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationWeatherGoodBadTrips.csv\"\n",
    "sdf_goodBadTripsByStationMonth = spark.read.csv(goodBadTripsByStationMonth_filepath).rdd.collectAsMap()\n",
    "for x in list(sdf_goodBadTripsByStationMonth)[0:3]:\n",
    "    print (\"key {}, value {} \".format(x,  sdf_goodBadTripsByStationMonth[x]))\n",
    "bc_goodBadTripsByStationMonth = sc.broadcast(sdf_goodBadTripsByStationMonth)\n",
    "\n",
    "# duration by station and month and good/bad weather\n",
    "goodBadDurationByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationWeatherGoodBadDuration.csv\"\n",
    "sdf_goodBadDurationByStationMonth = spark.read.csv(goodBadDurationByStationMonth_filepath).rdd.collectAsMap()\n",
    "for x in list(sdf_goodBadDurationByStationMonth)[0:3]:\n",
    "    print (\"key {}, value {} \".format(x,  sdf_goodBadDurationByStationMonth[x]))\n",
    "bc_goodBadDurationByStationMonth = sc.broadcast(sdf_goodBadDurationByStationMonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access the GOOD OR BAD weather use tripdate\n",
    "# bc_goodbadweatherdate.value.get(tripdate)\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def setweatherGoodTripIndex(month, startStationName):\n",
    "    \n",
    "    averageTrips = bc_tripsByStationMonth.value.get(month + \"-\" + startStationName)\n",
    "    goodTrips = bc_goodBadTripsByStationMonth.value.get(month + \"-\" + startStationName + \"GOOD\")\n",
    "    if goodTrips is None:\n",
    "    #    # if the dictionary doesn't have a good trips value, use the average trips\n",
    "        goodTrips = averageTrips\n",
    "    \n",
    "    index_value = float(goodTrips) / float(averageTrips)\n",
    "    return index_value\n",
    "\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def setweatherBadTripIndex(month : str, startStationName : str):\n",
    "    averageTrips = bc_tripsByStationMonth.value.get(month + \"-\" + startStationName)\n",
    "    badTrips = bc_goodBadTripsByStationMonth.value.get(month + \"-\" + startStationName + \"BAD\")\n",
    "    if badTrips is None:\n",
    "        # if the dictionary doesn't have a bad trips value, use the average trips\n",
    "        badTrips = averageTrips\n",
    "\n",
    "    index_value = float(badTrips) / float(averageTrips)\n",
    "    return index_value\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def setweatherGoodDurationIndex(month : str, startStationName : str):\n",
    "    averageDuration = bc_durationByStationMonth.value.get(month + \"-\" + startStationName)\n",
    "    goodDuration= bc_goodBadDurationByStationMonth.value.get(month + \"-\" + startStationName + \"GOOD\")\n",
    "    if goodDuration is None:\n",
    "        # if the dictionary doesn't have a good duration value, use the average trips\n",
    "        goodDuration = averageDuration\n",
    "                         \n",
    "    index_value = float(goodDuration) / float(averageDuration)\n",
    "    return index_value\n",
    "\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def setweatherBadDurationIndex(month : str, startStationName : str):\n",
    "    averageDuration = bc_durationByStationMonth.value.get(month + \"-\" + startStationName)\n",
    "    badDuration= bc_goodBadDurationByStationMonth.value.get(month + \"-\" + startStationName + \"BAD\")\n",
    "    if badDuration is None:\n",
    "        # if the dictionary doesn't have a bad duration value, use the average trips\n",
    "        badDuration = averageDuration\n",
    "\n",
    "    index_value = float(badDuration) / float(averageDuration)\n",
    "    return index_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the standard data file with only the columns I am interested in using:\n",
    "file = \"/project/ds5559/Summer2021_TeamBike/gt2017_stationName_withGoodBadWeather.parquet\" # FINAL DATA FILE\n",
    "sdf = spark.read.parquet(file).select('startStationName', 'month')\n",
    "\n",
    "sdf = sdf.withColumn(\"weatherGoodTripIndex\", setweatherGoodTripIndex(col(\"month\"), col(\"startStationName\")))\n",
    "sdf = sdf.withColumn(\"weatherBadTripIndex\", setweatherBadTripIndex(col(\"month\"), col(\"startStationName\")))\n",
    "sdf = sdf.withColumn(\"weatherGoodDurationIndex\", setweatherGoodDurationIndex(col(\"month\"), col(\"startStationName\")))\n",
    "sdf = sdf.withColumn(\"weatherBadDurationIndex\", setweatherBadDurationIndex(col(\"month\"), col(\"startStationName\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+--------------------+-------------------+------------------------+-----------------------+\n",
      "|startStationName|month|weatherGoodTripIndex|weatherBadTripIndex|weatherGoodDurationIndex|weatherBadDurationIndex|\n",
      "+----------------+-----+--------------------+-------------------+------------------------+-----------------------+\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "|6 Ave & Canal St|  Jan|  0.7619760479041916|0.23802395209580837|      1.0185203735615738|     0.9407115085355908|\n",
      "+----------------+-----+--------------------+-------------------+------------------------+-----------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[startStationName: string, month: string, weatherGoodTripIndex: double, weatherBadTripIndex: double, weatherGoodDurationIndex: double, weatherBadDurationIndex: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_distinct = sdf.select([\"startStationName\",\"month\", \"weatherGoodTripIndex\", \"weatherBadTripIndex\", \"weatherGoodDurationIndex\", \"weatherBadDurationIndex\"]).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sdf_distinct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_distinct.write.mode(\"overwrite\").csv('/home/hdm5s/ds5110/FinalProject/weatherIndexDistinctStationMonth.csv')\n",
    "sdf_distinct.write.mode(\"overwrite\").csv('/project/ds5559/Summer2021_TeamBike/weatherIndexDistinctStationMonth.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save csv's to use as broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat\n",
    "sdf_distinct.withColumn(\"key\", concat(col(\"startStationName\"),lit(\"-\"),col(\"month\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_weatherGoodTripIndex = sdf_distinct.withColumn(\"key\", concat(col(\"startStationName\"),lit(\"-\"),col(\"month\"))).select(\"key\",\"weatherGoodTripIndex\")\n",
    "sdf_weatherGoodTripIndex.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_weatherGoodTripIndex.write.mode(\"overwrite\").csv('/home/hdm5s/ds5110/FinalProject/weatherIndexDistinctStationMonth-GoodTripIndex.csv')\n",
    "sdf_weatherGoodTripIndex.write.mode(\"overwrite\").csv('/project/ds5559/Summer2021_TeamBike/weatherIndexDistinctStationMonth-GoodTripIndex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_weatherBadTripIndex = sdf_distinct.withColumn(\"key\", concat(col(\"startStationName\"),lit(\"-\"),col(\"month\"))).select(\"key\",\"weatherBadTripIndex\")\n",
    "sdf_weatherBadTripIndex.show()\n",
    "sdf_weatherBadTripIndex.write.mode(\"overwrite\").csv('/home/hdm5s/ds5110/FinalProject/weatherIndexDistinctStationMonth-BadTripIndex.csv')\n",
    "sdf_weatherBadTripIndex.write.mode(\"overwrite\").csv('/project/ds5559/Summer2021_TeamBike/weatherIndexDistinctStationMonth-BadTripIndex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_weatherGoodDurationIndex = sdf_distinct.withColumn(\"key\", concat(col(\"startStationName\"),lit(\"-\"),col(\"month\"))).select(\"key\",\"weatherGoodDurationIndex\")\n",
    "sdf_weatherGoodDurationIndex.show()\n",
    "sdf_weatherGoodDurationIndex.write.mode(\"overwrite\").csv('/home/hdm5s/ds5110/FinalProject/weatherIndexDistinctStationMonth-GoodDurationIndex.csv')\n",
    "sdf_weatherGoodDurationIndex.write.mode(\"overwrite\").csv('/project/ds5559/Summer2021_TeamBike/weatherIndexDistinctStationMonth-GoodDurationIndex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_weatherBadDurationIndex = sdf_distinct.withColumn(\"key\", concat(col(\"startStationName\"),lit(\"-\"),col(\"month\"))).select(\"key\",\"weatherBadDurationIndex\")\n",
    "sdf_weatherBadDurationIndex.show()\n",
    "sdf_weatherBadDurationIndex.write.mode(\"overwrite\").csv('/home/hdm5s/ds5110/FinalProject/weatherIndexDistinctStationMonth-BadDurationIndex.csv')\n",
    "sdf_weatherBadDurationIndex.write.mode(\"overwrite\").csv('/project/ds5559/Summer2021_TeamBike/weatherIndexDistinctStationMonth-BadDurationIndex.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of loading the good weather duration and bad weather duration index to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the broadcast variables from csv\n",
    "# duration by station and month\n",
    "durationByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationDuration.csv\"\n",
    "sdf_durationByStationMonth = spark.read.csv(durationByStationMonth_filepath).rdd.collectAsMap()\n",
    "#for x in list(sdf_durationByStationMonth)[0:3]:\n",
    "#    print (\"key {}, value {} \".format(x,  sdf_durationByStationMonth[x]))\n",
    "bc_durationByStationMonth = sc.broadcast(sdf_durationByStationMonth)\n",
    "\n",
    "# duration by station and month and good/bad weather\n",
    "goodBadDurationByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationWeatherGoodBadDuration.csv\"\n",
    "sdf_goodBadDurationByStationMonth = spark.read.csv(goodBadDurationByStationMonth_filepath).rdd.collectAsMap()\n",
    "#for x in list(sdf_goodBadDurationByStationMonth)[0:3]:\n",
    "#    print (\"key {}, value {} \".format(x,  sdf_goodBadDurationByStationMonth[x]))\n",
    "bc_goodBadDurationByStationMonth = sc.broadcast(sdf_goodBadDurationByStationMonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------------+------------------------+--------------+------------------------+--------------------+---------------------+------------+------------------+------------------+-------------------+-------+----------+---------+------+------+----------+--------+--------+--------+--------+----------+-------+-------+-------+-------+----------+------------+---+------+-----+--------+------------+----+---------+---------+---------+------------------------+-----------------------+\n",
      "|date      |hour|tripduration|starttime               |startStationId|startStationName        |startStationLatitude|startStationLongitude|endStationId|endStationName    |endStationLatitude|endStationLongitude|bikeid |usertype  |birthyear|gender|temp  |feels_like|temp_min|temp_max|pressure|humidity|wind_speed|rain_1h|rain_3h|snow_1h|snow_3h|clouds_all|weather_main|dow|day   |month|time_bin|peak_commute|year|3h_precip|1h_precip|precip   |weatherGoodDurationIndex|weatherBadDurationIndex|\n",
      "+----------+----+------------+------------------------+--------------+------------------------+--------------------+---------------------+------------+------------------+------------------+-------------------+-------+----------+---------+------+------+----------+--------+--------+--------+--------+----------+-------+-------+-------+-------+----------+------------+---+------+-----+--------+------------+----+---------+---------+---------+------------------------+-----------------------+\n",
      "|2018-10-01|0   |330         |2018-10-01 00:00:00.7010|293.0         |Lafayette St & E 8 St   |40.73020660529954   |-73.99102628231049   |504.0       |1 Ave & E 16 St   |40.73221853       |-73.98165557       |30493.0|Subscriber|1995     |1.0   |65.318|64.508    |63.754  |63.754  |1026    |77      |2.65      |0.0    |0.0    |0.0    |0.0    |1         |Clear       |2  |Monday|Oct  |Night   |non_peak    |2018|0.0      |0.0      |no_precip|1.029469188246277       |0.917077410339218      |\n",
      "|2018-10-01|0   |309         |2018-10-01 00:00:13.5780|433.0         |old - E 13 St & Avenue A|40.72955361         |-73.98057249         |394.0       |E 9 St & Avenue C |40.72521311       |-73.97768752       |33054.0|Subscriber|1967     |1.0   |65.318|64.508    |63.754  |63.754  |1026    |77      |2.65      |0.0    |0.0    |0.0    |0.0    |1         |Clear       |2  |Monday|Oct  |Night   |non_peak    |2018|0.0      |0.0      |no_precip|0.9918619978038982      |1.0216974694786225     |\n",
      "|2018-10-01|0   |734         |2018-10-01 00:00:16.6040|3472.0        |W 15 St & 10 Ave        |40.742753828659026  |-74.00747358798982   |285.0       |Broadway & E 14 St|40.73454567       |-73.99074142       |33400.0|Subscriber|1983     |2.0   |65.318|64.508    |63.754  |63.754  |1026    |77      |2.65      |0.0    |0.0    |0.0    |0.0    |1         |Clear       |2  |Monday|Oct  |Night   |non_peak    |2018|0.0      |0.0      |no_precip|1.0210878753898789      |0.9275030487716374     |\n",
      "|2018-10-01|0   |788         |2018-10-01 00:00:35.0670|417.0         |Barclay St & Church St  |40.71291224         |-74.01020234         |3472.0      |W 15 St & 10 Ave  |40.742753828659026|-74.00747358798982 |31140.0|Customer  |1988     |1.0   |65.318|64.508    |63.754  |63.754  |1026    |77      |2.65      |0.0    |0.0    |0.0    |0.0    |1         |Clear       |2  |Monday|Oct  |Night   |non_peak    |2018|0.0      |0.0      |no_precip|1.06287502512929        |0.8120096998487192     |\n",
      "|2018-10-01|0   |1136        |2018-10-01 00:00:38.1410|3699.0        |W 50 St & 9 Ave         |40.76360467795863   |-73.98917958140373   |495.0       |W 47 St & 10 Ave  |40.76269882       |-73.99301222       |20294.0|Subscriber|1969     |0.0   |65.318|64.508    |63.754  |63.754  |1026    |77      |2.65      |0.0    |0.0    |0.0    |0.0    |1         |Clear       |2  |Monday|Oct  |Night   |non_peak    |2018|0.0      |0.0      |no_precip|0.9948869065417824      |1.0131232743079661     |\n",
      "+----------+----+------------+------------------------+--------------+------------------------+--------------------+---------------------+------------+------------------+------------------+-------------------+-------+----------+---------+------+------+----------+--------+--------+--------+--------+----------+-------+-------+-------+-------+----------+------------+---+------+-----+--------+------------+----+---------+---------+---------+------------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the broadcast variables from csv\n",
    "# duration by station and month\n",
    "durationByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationDuration.csv\"\n",
    "sdf_durationByStationMonth = spark.read.csv(durationByStationMonth_filepath).rdd.collectAsMap()\n",
    "#for x in list(sdf_durationByStationMonth)[0:3]:\n",
    "#    print (\"key {}, value {} \".format(x,  sdf_durationByStationMonth[x]))\n",
    "bc_durationByStationMonth = sc.broadcast(sdf_durationByStationMonth)\n",
    "\n",
    "# duration by station and month and good/bad weather\n",
    "goodBadDurationByStationMonth_filepath = \"/project/ds5559/Summer2021_TeamBike/broadcastMonthStationWeatherGoodBadDuration.csv\"\n",
    "sdf_goodBadDurationByStationMonth = spark.read.csv(goodBadDurationByStationMonth_filepath).rdd.collectAsMap()\n",
    "#for x in list(sdf_goodBadDurationByStationMonth)[0:3]:\n",
    "#    print (\"key {}, value {} \".format(x,  sdf_goodBadDurationByStationMonth[x]))\n",
    "bc_goodBadDurationByStationMonth = sc.broadcast(sdf_goodBadDurationByStationMonth)\n",
    "\n",
    "# to add columns with the GOOD and bad weather duration indexes\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def setweatherGoodDurationIndex(month : str, startStationName : str):\n",
    "    averageDuration = bc_durationByStationMonth.value.get(month + \"-\" + startStationName)\n",
    "    goodDuration= bc_goodBadDurationByStationMonth.value.get(month + \"-\" + startStationName + \"GOOD\")\n",
    "    if goodDuration is None:\n",
    "        # if the dictionary doesn't have a good duration value, use the average trips\n",
    "        goodDuration = averageDuration\n",
    "                         \n",
    "    index_value = float(goodDuration) / float(averageDuration)\n",
    "    return index_value\n",
    "\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def setweatherBadDurationIndex(month : str, startStationName : str):\n",
    "    averageDuration = bc_durationByStationMonth.value.get(month + \"-\" + startStationName)\n",
    "    badDuration= bc_goodBadDurationByStationMonth.value.get(month + \"-\" + startStationName + \"BAD\")\n",
    "    if badDuration is None:\n",
    "        # if the dictionary doesn't have a bad duration value, use the average trips\n",
    "        badDuration = averageDuration\n",
    "\n",
    "    index_value = float(badDuration) / float(averageDuration)\n",
    "    return index_value\n",
    "\n",
    "# load the standard data file with only the columns I am interested in using:\n",
    "# THE IS THE FINAL DATA FILE\n",
    "file = \"/project/ds5559/Summer2021_TeamBike/gt2017_bikeweather.parquet\" \n",
    "sdf = spark.read.parquet(file)\n",
    "\n",
    "sdf = sdf.withColumn(\"weatherGoodDurationIndex\", setweatherGoodDurationIndex(col(\"month\"), col(\"startStationName\")))\n",
    "sdf = sdf.withColumn(\"weatherBadDurationIndex\", setweatherBadDurationIndex(col(\"month\"), col(\"startStationName\")))\n",
    "\n",
    "sdf.show(5, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
