{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Model Fitting\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### By: TeamBike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create session: 16 cores, 128 GB RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process is similar to the logistic regression model, though model never ran with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"models\") \\\n",
    "    .config('spark.cores.max', '16') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"models\") \\\n",
    "    .config(\"spark.executor.memory\", '80g') \\\n",
    "    .config('spark.executor.cores', '16') \\\n",
    "    .config('spark.cores.max', '16') \\\n",
    "    .config(\"spark.driver.memory\",'40g') \\\n",
    "    .getOrCreate()\n",
    "'''\n",
    "\n",
    "#spark = SparkSession \\\n",
    "#    .builder.getOrCreate()\n",
    "\n",
    "\n",
    "sc = SparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This model is essentially a challenger model to the logistic regression we also ran. The EDA exists in the `log_reg_NBmodel.ipynb` file, which goes over the neighborhood mapping and start-to-end distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipeline is unchanged from the logistic-regression file, so no need to run this code chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "cats = ['day', 'time_bin', 'peak_commute', 'month', 'start_neighborhood']\n",
    "\n",
    "# The index of string vlaues multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in cats\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")\n",
    "\n",
    "label_indexer = StringIndexer(inputCol='end_neighborhood', outputCol= 'n_index')\n",
    "\n",
    "label_encoder = OneHotEncoder(inputCol='n_index',outputCol= 'label')\n",
    "                              \n",
    "#label_assember = VectorAssembler(inputCols=,outputCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [label_indexer] + encoders+ [label_encoder] + [assembler])\n",
    "model=pipeline.fit(df_group)\n",
    "transformed = model.transform(df_group)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transformed data (output in Logistic Regression file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformed.write.parquet(\"pipelined_data_NBs.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = spark.read.parquet(\"pipelined_data_NBs.parquet\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data: Using 60/20/20 split is more than sufficient -- the model takes a very long time to fit due to the number of observations, even with 60% as training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "#randomly split data into training and test dataset\n",
    "(train_data, test_data, hold_out) = transformed.randomSplit([0.60, 0.20, 0.20], seed = 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the random-forest model. I do not use CV as this model takes a very long time to run and I often ran out of resources. Although CV may have helped slightly, I do not think it would have provided much of a boost. Modeling end neighborhoods is difficult due to the fact that most rides are intra-neighborhood, and as such, most predictions predict that a ride will end in the same area where it started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train RandomForest model\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"n_index\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "predictions = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.368\n",
      "StringIndexerModel: uid=StringIndexer_9e604145725b, handleInvalid=error\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"n_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = rf_model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start_neighborhood',\n",
       " 'end_neighborhood',\n",
       " 'day',\n",
       " 'time_bin',\n",
       " 'peak_commute',\n",
       " 'month',\n",
       " 'hour',\n",
       " 'day_indexed',\n",
       " 'time_bin_indexed',\n",
       " 'peak_commute_indexed',\n",
       " 'month_indexed',\n",
       " 'start_neighborhood_indexed',\n",
       " 'n_index',\n",
       " 'day_indexed_encoded',\n",
       " 'time_bin_indexed_encoded',\n",
       " 'peak_commute_indexed_encoded',\n",
       " 'month_indexed_encoded',\n",
       " 'start_neighborhood_indexed_encoded',\n",
       " 'label',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.save(\"rf_nbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions.select('start_neighborhood', 'end_neighborhood', 'day', 'time_bin', 'peak_commute', 'month', 'hour', 'n_index', 'prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See that the prediction accuracy is about 63%. I will cover model analysis in another notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6326516755198247"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select('start_neighborhood', 'end_neighborhood', 'day', 'time_bin', 'peak_commute', 'month', 'hour', 'n_index', 'prediction')\\\n",
    ".where(preds.start_neighborhood == preds.end_neighborhood).count()/preds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model files are saved for analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110",
   "language": "python",
   "name": "ds5110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
